{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network and Adversarial Attacks\n",
    "\n",
    "En el siguiente código se crea una red neuronal convolucional que identifica dígitos escritos a mano para posteriormente engañarla usando un white box adversarial attack conocido como FGSM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importaciones y definición de CPU o GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" # Poner -1 para CPU y comentar para GPU\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from random import randrange\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU'))) # Imprimimos qué usará como comprobación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "A continuación descargamos el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Cargamos el dataset y lo dividimos en entrenamiento y entreno\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\n",
    "\n",
    "# Reducimos el dataset para hacer que la compilación sea más rápida\n",
    "NUM_TRAINSET = 60000    # El número máximo es 60000\n",
    "NUM_TESTSET = 10000       # El número máximo es 10000\n",
    "\n",
    "train_images, train_labels = train_images[0:NUM_TRAINSET, :, :], train_labels[0:NUM_TRAINSET]\n",
    "test_images, test_labels = test_images[0:NUM_TESTSET, :, :], test_labels[0:NUM_TESTSET]\n",
    "\n",
    "# Importante: normalizar los píxeles entre 0 y 1. También se puede usar el método normalize\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprobamos el formato\n",
    "\n",
    "Elegimos una imagen y mostramos su imagen y su etiqueta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_imagen = 7  \n",
    "\n",
    "plt.imshow(train_images[num_imagen] ,cmap=plt.cm.binary)\n",
    "plt.xlabel(train_labels[num_imagen])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape\n",
    "La red espera una entrada de la forma (60000, 28, 28, 1), de forma que tenemos que modificar la entrada a eso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape(NUM_TRAINSET,28,28,1)\n",
    "test_images = test_images.reshape(NUM_TESTSET,28,28,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un modelo secuencial, es decir, la información fluirá de izquierda a derecha\n",
    "model = models.Sequential()\n",
    "\n",
    "# Añadimos las capas convolucionaes\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "# Y posteriormente, las pasamos a capas lineales\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu')) # Que escojamos 64 neuronas no tiene nada que ver con lo anterior\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Echemos un vistazo a cómo nos queda la red\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entrenamiento = model.fit(train_images, train_labels, batch_size = 32, epochs=3, validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de resultados\n",
    "\n",
    "Comprobamos ahora en función de los datos de pérdida y precisión si sería beneficial aumentar las épocas o no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos la pérdida y la precisión\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel(\"Época\")\n",
    "plt.plot(entrenamiento.history['loss'], label= \"Loss\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Precisión\")\n",
    "plt.xlabel(\"Época\")\n",
    "plt.plot(entrenamiento.history['accuracy'], label= \"Precisión\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En base a la gráfica se observa que el valor ideal de épocas es 3 (empezamos en 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar datos en txt\n",
    "\n",
    "Guardamos los resultados en un txt para poder crear las figuras en latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un vector x con el número de epocas desde 1\n",
    "x = np.linspace(1, len(entrenamiento.history['loss']), num =len(entrenamiento.history['loss']))\n",
    "        \n",
    "# Formamos una matriz y la transponemos para que los vectores se queden como columnas\n",
    "matriz = np.array([x, entrenamiento.history['loss']])\n",
    "matriz = matriz.transpose()\n",
    "        \n",
    "# Cuardamos el txt definiendo las columnas, el formato, el delimitador y evitando que aparezca el hastagh\n",
    "np.savetxt(\"mnistbasic_loss.txt\", matriz, header = 'Epoch Loss', fmt='%1.10f', delimiter = ' ', comments='')\n",
    "\n",
    "# Repetimos para la precisión\n",
    "matriz = np.array([x, entrenamiento.history['accuracy']])\n",
    "matriz = matriz.transpose()\n",
    "np.savetxt(\"mnist_basic_accuracy.txt\", matriz, header = 'Epoch Accuracy', fmt='%1.10f', delimiter = ' ', comments='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicción de resultados\n",
    "\n",
    "Comprobamos que las imágenes introducidas corresponden con los valores predichos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_imagen = 0\n",
    "\n",
    "plt.imshow(test_images[num_imagen], cmap=plt.cm.binary)\n",
    "plt.xlabel(test_labels[num_imagen])\n",
    "plt.show()\n",
    "\n",
    "print(\"La clase predicha es:\", np.argmax(model.predict([test_images])[num_imagen]), \"con una confianza del\", \n",
    "      round(np.max(model.predict([test_images])[num_imagen])*100, 2), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación del dataset FGSM\n",
    "### Crear las perturbaciones\n",
    "Primero calculamos las perturbaciones para el dataset escogido, de forma que podemos realizar el FGSM sobre el test para realizar pruebas o sobre el train para entrenar posteriormente a la red con él"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introducimos el dataset del que queremos imágenes adversariales. De momento es test, luego ya en la defensa pondremos train\n",
    "dataset = \"test\"\n",
    "   \n",
    "perturbaciones_testset = FGSM(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de las adversarial images\n",
    "Una vez tenemos las perturbaciones del dataset escogido, lo recorremos para los valores de epsilon deseados obteniendo un vector de predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos los valores de epsilon para los que queremos crear el dataset\n",
    "epsilons = np.array([0.01, 0.05, 0.1, 0.15, 0.2, 0.3, 0.4, 0.5])\n",
    "\n",
    "# Creamos nuestro vector vacío con las predicciones\n",
    "predicciones = np.empty([len(epsilons), NUM_TESTSET, 10])\n",
    "\n",
    "# Creamos nuestro vector vacío que contendrá el testset\n",
    "adversarial_testset = np.empty([len(epsilons), NUM_TESTSET, 28, 28, 1])\n",
    "            \n",
    "# Recorremos los datasets para cada uno de esos valores de epsilon    \n",
    "for i in range(len(epsilons)):\n",
    "    # Creamos el adversarial dataset\n",
    "    adversarial_testset[i] = creacion_adversarial_dataset(epsilons[i], dataset, perturbaciones_testset)   \n",
    "\n",
    "    # Calculamos las métricas\n",
    "    predicciones[i] = model.predict([adversarial_testset[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprobación de que se han creado correctamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos la forma\n",
    "print(\"La forma del vector predicciones es:\", predicciones.shape)\n",
    "print(\"La forma del adversarial testset es:\", adversarial_testset.shape)\n",
    "    \n",
    "# Imprimimos una imagen aleatoria con un valor de epsilon aleatorio\n",
    "num_imagen = randrange(NUM_TESTSET)\n",
    "valor = randrange(len(epsilons))\n",
    "    \n",
    "imagen_original = test_images[num_imagen]\n",
    "perturbacion = perturbaciones_testset[num_imagen] * 0.5 + 0.5\n",
    "imagen = adversarial_testset[valor][num_imagen]\n",
    "    \n",
    "# Llamamos a la funciones de impresión y predicción\n",
    "impresion_imagen_adversarial(imagen_original, perturbacion, imagen, epsilons[valor])\n",
    "prediccion_adversarial(imagen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos los resultados. IMPORTANTE: Poner 1 si estamos trabajando con el dataset adversativo ó 0 si no, ya que creará los txt\n",
    "# con el nombre correspondiente\n",
    "metricas(predicciones, NUM_TESTSET, adversativo = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defensa frente a ataques adversativos\n",
    "A continuación tenemos que realizar un proceso muy parecido al anterior pero ahora con el trainset, de forma que obtendremos 60000 imágenes adversativas para el valor o valores de epsilon especificados. Esas imágenes las añadiremos al dataset inicial y entrenaremos a nuestra red con los conjuntos originales y los adversariales de forma que la precisión debería volver a subir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de las perturbaciones del trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"train\"\n",
    "   \n",
    "perturbaciones_trainset = FGSM(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación del trainset adversarial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos los valores de epsilon para los que queremos crear el dataset\n",
    "# Tarda un huevo, así que, de momento, preferiblemente sólo 1. Pongo 0.4 porque es el que hace que sea básicamente random\n",
    "epsilon = np.array([0.4])\n",
    "\n",
    "# Creamos nuestro vector vacío que contrendrá el dataset\n",
    "adversarial_trainset = np.empty([len(epsilon), NUM_TRAINSET, 28, 28, 1])\n",
    "            \n",
    "# Recorremos los datasets para cada uno de esos valores de epsilon    \n",
    "for i in range(len(epsilon)):\n",
    "    # Creamos el adversarial dataset\n",
    "    adversarial_trainset[i] = creacion_adversarial_dataset(epsilon[i], dataset, perturbaciones_trainset)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprobación de que se ha creado correctamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos la forma\n",
    "print(\"La forma del adversarial trainset es:\", adversarial_trainset.shape)\n",
    "    \n",
    "# Imprimimos una imagen aleatoria con un valor de epsilon aleatorio\n",
    "valor = randrange(len(epsilon))\n",
    "num_imagen = randrange(NUM_TRAINSET)\n",
    "\n",
    "imagen_original = train_images[num_imagen]\n",
    "perturbacion = perturbaciones_trainset[num_imagen]\n",
    "imagen = adversarial_trainset[valor][num_imagen]\n",
    "\n",
    "# Llamamos a la funciones de impresión y predicción\n",
    "impresion_imagen_adversarial(imagen_original, perturbacion, imagen, epsilon[valor])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación del nuevo conjunto de entrenamiento y test con las imágenes adversariales\n",
    "Hasta ahora tenemos nuestro adversarial trainset y el adversarial dataset. Ahora tenemos que definir qué imágenes queremos de dentro de esos conjuntos, así como establecer sus correctas etiquetas para entrenar posteriormente la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación del conjunto de entrenamiento\n",
    "# Lo inicializamos con las imágenes originales\n",
    "trainset_defensa = train_images\n",
    "trainset_defensa_labels = train_labels\n",
    "\n",
    "# Y vamos añadiendo los epsilons establecidos, aunque típicamente será sólo 1\n",
    "for i in range(len(epsilon)):\n",
    "    trainset_defensa = np.concatenate((trainset_defensa, adversarial_trainset[i]))\n",
    "    trainset_defensa_labels = np.concatenate((trainset_defensa_labels, train_labels))\n",
    "\n",
    "# Imprimimos las formas para comprobar que son las correctas\n",
    "print(trainset_defensa.shape)\n",
    "print(trainset_defensa_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación del conjunto de test. Lo creamos para los mismos epsilons\n",
    "testset_defensa = test_images\n",
    "testset_defensa_labels = test_labels\n",
    "\n",
    "# Encontramos la posición de epsilon en el vector de epsilons\n",
    "posicion_epsilon = int(np.where(epsilons == epsilon)[0])\n",
    "\n",
    "# Y vamos añadiendo el testset que corresponde al mismo valor de epsilon que en el caso anterior, que es la posición 6\n",
    "testset_defensa = np.concatenate((testset_defensa, adversarial_testset[posicion_epsilon]))\n",
    "testset_defensa_labels = np.concatenate((testset_defensa_labels, test_labels))\n",
    "\n",
    "# Imprimimos las formas para comprobar que son las correctas\n",
    "print(testset_defensa.shape)\n",
    "print(testset_defensa_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento del modelo con el nuevo dataset adversarial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Antes de entrenar al modelo, volvemos a compilarlo para asegurarnos de que se reinicia\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
    "\n",
    "# Y entrenamos\n",
    "entrenamiento = model.fit(trainset_defensa, trainset_defensa_labels, epochs=3, validation_data=(testset_defensa, testset_defensa_labels), batch_size = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cálculo de las nuevas predicciones del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos nuestro vector vacío con las predicciones\n",
    "predicciones = np.empty([len(epsilons), NUM_TESTSET, 10])\n",
    "            \n",
    "# Recorremos los datasets para cada uno de esos valores de epsilon    \n",
    "for i in range(len(epsilons)):\n",
    "\n",
    "    # Calculamos las métricas\n",
    "    predicciones[i] = model.predict([adversarial_testset[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de métricas con el nuevo modelo\n",
    "Repetimos los pasos anteriores para comprobar si efectivamente hemos mejorado la precisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volvemos a analizar el modelo pero especificamos que estamos con el dataset adversaativo para que cree los txt\n",
    "metricas(predicciones, NUM_TESTSET, adversativo = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones utilizadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Creación de la perturbación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FGSM(dataset):\n",
    "    \n",
    "    # Definimos el dataset a utilizar, si el train o el test\n",
    "    if dataset == \"test\":\n",
    "        dataset = test_images\n",
    "        labels = test_labels\n",
    "    else:\n",
    "        dataset = train_images\n",
    "        labels = train_labels\n",
    "    \n",
    "    # Creamos un vector vacío para almacenar las perturbaciones\n",
    "    perturbaciones = np.empty([len(dataset), 28, 28, 1])\n",
    "    \n",
    "    # Recorremos las imágenes del dataset elegido\n",
    "    for num_imagen in range(len(dataset)):\n",
    "        # Expandimos la dimensión\n",
    "        imagen = dataset[num_imagen]\n",
    "        imagen = np.expand_dims(imagen, axis = 0)\n",
    "        label = labels[num_imagen]\n",
    "\n",
    "        # Convertimos a tensor\n",
    "        imagen = tf.convert_to_tensor(imagen, dtype = tf.float32)\n",
    "\n",
    "        # Definimos la función de pérdida\n",
    "        funcion_loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "        # Creamos la perturbacion\n",
    "        def crear_perturbacion(imagen, label):\n",
    "            with tf.GradientTape() as tape:\n",
    "                tape.watch(imagen)\n",
    "                prediccion = model(imagen)\n",
    "                loss = funcion_loss(label, prediccion)\n",
    "\n",
    "            gradiente = tape.gradient(loss, imagen)\n",
    "            signo_grad = tf.sign(gradiente)\n",
    "            return signo_grad\n",
    "\n",
    "        # Llamamos a la función\n",
    "        perturbaciones[num_imagen] = crear_perturbacion(imagen, label)\n",
    "\n",
    "    return perturbaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación del dataset adversarial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creacion_adversarial_dataset(epsilon, dataset, perturbaciones):\n",
    "    \n",
    "     # Definimos el dataset a utilizar, si el train o el test\n",
    "    if dataset == \"test\":\n",
    "        dataset = test_images\n",
    "        labels = test_labels\n",
    "    else:\n",
    "        dataset = train_images\n",
    "        labels = train_labels\n",
    "    \n",
    "    # Creamos el adversarial dataset vacío\n",
    "    adversarial_dataset = np.empty([len(dataset), 28, 28, 1])\n",
    "    \n",
    "    # Recorremos el dataset original y le vamos sumando el vector de perturbaciones creado anteriormente\n",
    "    for num_imagen in range(len(dataset)):\n",
    "        adversarial_dataset[num_imagen] = dataset[num_imagen] + epsilon * (perturbaciones[num_imagen] * 0.5 + 0.5)\n",
    "        adversarial_dataset[num_imagen] = tf.clip_by_value(adversarial_dataset[num_imagen], -1, 1)\n",
    "    \n",
    "    return adversarial_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impresión del dataset adversarial para cada epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impresion_imagen_adversarial(imagen_original, perturbacion, imagen, epsilon):\n",
    "        \n",
    "    # Imprimimos una imagen con su perturbación y su imagen adversarial resultante\n",
    "    plt.figure(figsize=(15,4))\n",
    "\n",
    "    plt.subplot(131)\n",
    "    plt.title(\"Imagen original\")\n",
    "    plt.imshow(imagen_original, cmap=plt.cm.binary)\n",
    "\n",
    "    plt.subplot(132)\n",
    "    plt.title(\"Perturbación\")\n",
    "    plt.imshow(perturbacion, cmap=plt.cm.binary)\n",
    "\n",
    "    plt.subplot(133)\n",
    "    plt.title(\"Imagen adversarial epsilon = {}\".format(epsilon))\n",
    "    plt.imshow(imagen, cmap=plt.cm.binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicción de una imagen adversarial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediccion_adversarial(imagen):\n",
    "    # Primero expandimos una dimensión\n",
    "    imagen = np.expand_dims(imagen, axis = 0)\n",
    "    \n",
    "    # Y realizamos la predicción\n",
    "    print(\"La predicción es:\", np.argmax(model.predict(imagen)), \"con una confianza del\", \n",
    "          round(np.max(model.predict(imagen)),2)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metricas(predicciones, numero_imagenes, adversativo):\n",
    "\n",
    "    # Creamos los vectores globales\n",
    "    aciertos_epsilon = np.empty(len(epsilons)) \n",
    "    fallos_epsilon = np.empty(len(epsilons))\n",
    "    precisiones = np.empty(len(epsilons))\n",
    "    confianzas = np.empty(len(epsilons)) \n",
    "\n",
    "    # Recorremos los epsilons\n",
    "    for i in range(len(epsilons)):\n",
    "        aciertos = 0\n",
    "        confianza = 0\n",
    "\n",
    "        # Recorremos las predicciones\n",
    "        for j in range(numero_imagenes):\n",
    "            prediccion = np.argmax(predicciones[i][j])\n",
    "            confianza = confianza + np.max(predicciones[i][j])\n",
    "            if prediccion == test_labels[j]:\n",
    "                aciertos += 1\n",
    "\n",
    "        # Calculamos las métricas para cada epsilon\n",
    "        fallos = numero_imagenes - aciertos\n",
    "        precision = (aciertos / numero_imagenes) * 100\n",
    "        confianza = (confianza / numero_imagenes) * 100\n",
    "        print(\"Para epsilon = {}: Aciertos = {}, Fallos = {} luego Precisión = {:0.2f} % con una confianza media del {:0.2f} %\"\n",
    "              . format(epsilons[i], aciertos, fallos, precision, confianza))\n",
    "\n",
    "        # Creamos un vector con todas las métricas para poder realizar gráficas\n",
    "        aciertos_epsilon[i] = aciertos\n",
    "        fallos_epsilon[i] = fallos\n",
    "        precisiones[i] = precision\n",
    "        confianzas[i] = confianza\n",
    "\n",
    "    # Creamos gráficas de los resultados\n",
    "    plt.figure(figsize=(20,4))\n",
    "\n",
    "    plt.subplot(141)\n",
    "    plt.title(\"Aciertos\")\n",
    "    plt.xlabel(\"Epsilon\")\n",
    "    plt.ylabel(\"Nº aciertos\")\n",
    "    plt.plot(epsilons, aciertos_epsilon)\n",
    "\n",
    "    plt.subplot(142)\n",
    "    plt.title(\"Fallos\")\n",
    "    plt.xlabel(\"Epsilon\")\n",
    "    plt.ylabel(\"Nº fallos\")\n",
    "    plt.plot(epsilons, fallos_epsilon)\n",
    "\n",
    "    plt.subplot(143)\n",
    "    plt.title(\"Precisión\")\n",
    "    plt.xlabel(\"Epsilon\")\n",
    "    plt.ylabel(\"Porcentaje\")\n",
    "    plt.plot(epsilons, precisiones)\n",
    "\n",
    "    plt.subplot(144)\n",
    "    plt.title(\"Confianza\")\n",
    "    plt.xlabel(\"Epsilon\")\n",
    "    plt.ylabel(\"Porcentaje\")\n",
    "    plt.plot(epsilons, confianzas)\n",
    "    \n",
    "    # Guardamos todos los datos en un txt\n",
    "    if adversativo == 0:\n",
    "        nombre = \"mnist_test_\"\n",
    "    else:\n",
    "        nombre = \"mnist_adversarial_\"\n",
    "    \n",
    "    # Formamos una matriz y la transponemos para que los vectores se queden como columnas\n",
    "    matriz = np.array([epsilons, aciertos_epsilon])\n",
    "    matriz = matriz.transpose()\n",
    "    np.savetxt(nombre + \"aciertos.txt\", matriz, header = 'Epsilon Aciertos', fmt='%1.10f', delimiter = ' ', comments='')\n",
    "                       \n",
    "    matriz = np.array([epsilons, fallos_epsilon])\n",
    "    matriz = matriz.transpose()\n",
    "    np.savetxt(nombre + \"fallos.txt\", matriz, header = 'Epsilon Fallos', fmt='%1.10f', delimiter = ' ', comments='')\n",
    "                                          \n",
    "    matriz = np.array([epsilons, precisiones])\n",
    "    matriz = matriz.transpose()\n",
    "    np.savetxt(nombre + \"accuracy.txt\", matriz, header = 'Epsilon Accuracy', fmt='%1.10f', delimiter = ' ', comments='')\n",
    "                                                             \n",
    "    matriz = np.array([epsilons, confianzas])\n",
    "    matriz = matriz.transpose()\n",
    "    np.savetxt(nombre + \"confianza.txt\", matriz, header = 'Epsilon Confianza', fmt='%1.10f', delimiter = ' ', comments='')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
